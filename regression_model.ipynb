{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required python packages\n",
    "import pandas as pd\n",
    "import importlib\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "from keras import models\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required helper functions\n",
    "from helper_functions import methods_prediction as pred_meth\n",
    "from helper_functions import methods_PrNrn as meth\n",
    "from helper_functions import control_methods as meth_control\n",
    "from helper_functions import SQA_preprocessing as SQA_prepro\n",
    "from helper_functions import evaluation_metrics as eval_metr\n",
    "from helper_functions import feature_engineering as feature_eng\n",
    "from helper_functions import test_grids\n",
    "from helper_functions import parametergrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the datasets to test/predict on\n",
    "df_sqa = pd.read_csv(params.filepath_project_folder + '\\SQA_full_prepro_data.csv', index_col=0)\n",
    "df_features_full = pd.read_csv(params.filepath_project_folder + '\\\\features_full.csv', index_col=0)\n",
    "df_features_full_reduced = pd.read_csv(params.filepath_project_folder + '\\\\features_full_reduced.csv', index_col=0)\n",
    "df_features_kaco = pd.read_csv(params.filepath_project_folder + '\\\\features_kaco.csv', index_col=0)\n",
    "df_features_kaco_reduced = pd.read_csv(params.filepath_project_folder + '\\\\features_kaco_reduced.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save parameters\n",
    "\n",
    "model_id = '\\Regr'\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "filepath_mod = ##path to prediction model\n",
    "filepath_res = ##path to prediction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the datasets to iterate through feature params and provide names for the datasets\n",
    "\n",
    "feature_dfs = [df_features_full_reduced]\n",
    "feature_dfs_names = ['Features_all_reduced']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the paramtergrids for the meta paramters and the model parameters\n",
    "importlib.reload(test_grids)\n",
    "importlib.reload(parametergrids)\n",
    "\n",
    "model_paramgrid = parametergrids.paramgrid_regression_NN\n",
    "meta_paramgrid = parametergrids.meta_paramgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty dataframe to store the prediction results\n",
    "df_results = pd.DataFrame(columns=['feature_set','model', 'cut_off_high', 'cut_off_low','delete', 'threshhold_late', 'target_col', 'sampling', \n",
    "                                   'sample_frac', 'cv_num', 'model_params', 'y_test', 'y_pred', 'mse', 'rmse', 'mae', 'mape', 'rae', 'r_squared', \n",
    "                                   'adj_r_squared', 'median_abs_error', 'timestamp'])\n",
    "\n",
    "df_results_reg_NN = pd.DataFrame(columns=['layer_depth', 'layer_architecture', 'input_activation_function', \n",
    "                                       'input_neurons', 'dropout', 'hidden_neurons', 'hidden_activation_function', \n",
    "                                       'output_activation_function', 'output_neurons', 'optimizer', 'epochs', \n",
    "                                       'batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the prediction\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "importlib.reload(meth_control)\n",
    "\n",
    "# Initiate counter\n",
    "c = 0\n",
    "feature_set_counter = 0\n",
    "\n",
    "# Iterate over the datasets to compare\n",
    "for df_features in feature_dfs:\n",
    "    feature_set_counter = feature_set_counter+1\n",
    "\n",
    "    # Iterate over the meta-parametergrid, e.g. sampling algorithms\n",
    "    for i in range (0, len(meta_paramgrid)):\n",
    "        print('- Meta-Parameter run ' + str(i+1) + '/' + str(len(meta_paramgrid)))\n",
    "    \n",
    "        # Iterate over the model parameter grid (model hyperparameter)\n",
    "        for j in range (0, len(model_paramgrid)):\n",
    "            print('- Model-Parameter run ' + str(j+1) + '/' + str(len(model_paramgrid)))\n",
    "            \n",
    "            # Call method to perform the regression task\n",
    "            models, mse, rmse, mae, mape, rae, r_squared, adj_r_squared, median_abs_error, y_preds, y_tests, model_results = meth_control.perform_regression(\n",
    "                                            df_features, df_sqa, meta_paramgrid[i], model_paramgrid[j])\n",
    "            \n",
    "            # Store results\n",
    "            df_results.loc[c] = [feature_dfs_names[feature_set_counter-1], 'regression_NN', meta_paramgrid[i]['cut_off_high'], \n",
    "                                         meta_paramgrid[i]['cut_off_low'], meta_paramgrid[i]['delete'],\n",
    "                                         meta_paramgrid[i]['threshhold_late'], meta_paramgrid[i]['target_col'], \n",
    "                                         meta_paramgrid[i]['sampling'], meta_paramgrid[i]['sample_frac'], \n",
    "                                         meta_paramgrid[i]['cv_num'],\n",
    "                                         model_paramgrid[j], y_tests, y_preds,\n",
    "                                         mse, rmse, mae, mape, rae, r_squared, adj_r_squared,\n",
    "                                         median_abs_error, time.strftime(\"%Y%m%d-%H%M%S\")]\n",
    "            c=c+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide for best regression params\n",
    "# Hidden activation function: relu outperformns sigmoid\n",
    "# Output activation function: linear outperforms sigmoid, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw results\n",
    "\n",
    "filename = model_id + timestr + '_results.csv'\n",
    "df_results.to_csv(filepath_res + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['mse', 'rmse', 'mae', 'mape', 'rae', 'r_squared', 'adj_r_squared', 'median_abs_error']:\n",
    "    for row in range(0, len(df_results)):\n",
    "        mean_value = sum(df_results.iloc[row][col])/len(df_results.iloc[row][col])\n",
    "        df_results[col][row] = mean_value\n",
    "        \n",
    "filename = model_id + timestr + '_results_edit.csv'\n",
    "df_results.to_csv(filepath_res + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for plotting\n",
    "c = 0\n",
    "\n",
    "for r in range (0, len(feature_dfs_names)):\n",
    "    \n",
    "    y_pred = []\n",
    "    for i in range(0, len(df_results.loc[r]['y_pred'][c])):\n",
    "        y_pred.append(df_results.loc[r]['y_pred'][c][i][0]) \n",
    "        \n",
    "    y_test = []\n",
    "    for i in range(0, len(df_results.loc[r]['y_test'][c])):\n",
    "        y_test.append(df_results.loc[r]['y_test'][c][i][0])\n",
    "\n",
    "    df_line_plot = pd.DataFrame(y_pred, columns=['y_pred'])\n",
    "    df_line_plot['y_test'] = y_test\n",
    "    df_line_plot['delta'] = df_line_plot['y_test'] - df_line_plot['y_pred']\n",
    "\n",
    "    filename = model_id + timestr + '_' + feature_dfs_names[r] +'_plotting.csv'\n",
    "    df_line_plot.to_csv(filepath_res + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot I-Cart comparing the true value, the predicted value and the delta\n",
    "f = plt.figure()\n",
    "\n",
    "plt.title('Model Performance', color='black')\n",
    "\n",
    "df_line_plot.plot(kind='line', ax=f.gca(), figsize=(16,10))\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot boxplots of the distribution of true value, prediction and delta\n",
    "df_line_plot.boxplot(figsize=(2,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a plotly graph to visualize regression results\n",
    "# I-Chart that compares true value, prediction and delta\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "x = ['Test', 'Pred', 'Delta']\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Box(y=df_line_plot['y_test'], name='F1: On-Time Class'))\n",
    "fig.add_trace(go.Box(y=df_line_plot['y_pred'], name='F1: On-Time Class'))\n",
    "fig.add_trace(go.Box(y=df_line_plot['delta'], name='F1: On-Time Class'))\n",
    "\n",
    "    \n",
    "fig.update_layout(\n",
    "    title='Score for different models',\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=20\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        autorange=True,\n",
    "        showgrid=True,\n",
    "        zeroline=True,\n",
    "        title='SQA'\n",
    "    ))\n",
    "\n",
    "\n",
    "plotly.offline.plot(fig, auto_open=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
